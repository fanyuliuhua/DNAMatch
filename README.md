# DNAMatch: An Ultra-Fast and Memory-Efficient Deep Learning Framework for Aligning Ultra-Long DNA Fragments
## Introduction
Efficient and accurate alignment of DNA fragments is fundamental to genomics research. With the advent of advanced sequencing technologies, the length of sequencing reads and assembled contigs has increased significantly, posing substantial challenges for existing alignment algorithms. These methods often struggle with megabase-scale DNA fragments due to the computational burden of global searches and exhaustive chromosomal queries. To address this, we propose DNAMatch, a novel alignment framework that integrates the DNABERT2 pre-trained model for feature extraction with a deep residual network for chromosome identification. DNAMatch introduces a chromosome pre-localization strategy, which effectively narrows the search space and significantly reduces the memory footprint required by the downstream aligner, minimap2. This design enables fast and precise alignment of ultra-long DNA fragments. Benchmarking on simulated ultra-long reads from multiple model organisms—including human, Drosophila melanogaster, and Arabidopsis thaliana—demonstrates that DNAMatch achieves 98–99% accuracy in chromosome identification, accelerates the alignment process by 52.7%, and reduces memory usage by 73%. Importantly, when applied to downstream structural variation detection, DNAMatch maintains high accuracy, with only a marginal 0.05% decrease in F1-score compared to the standard minimap2 pipeline. These results highlight DNAMatch as a powerful and efficient tool for aligning ultra-long reads and analyzing complex genomes.
## Method
![Overview of the DNAMatch framework](https://github.com/fanyuliuhua/DNAMatch/blob/main/data/DNAMatch.png)
###### Figure 1. Overview of the DNAMatch framework. (A) Workflow of the DNAMatch learning pipeline. The input DNA sequence is first encoded using BPE, resulting in M tokens. (B) Pre-trained embedding module, where the BPE-encoded sequence is fed into the DNABERT2 model to obtain a high-dimensional representation. (C) Chromosome identification module, which uses a ResNet-based classifier to predict the chromosomal origin. Here, L denotes the number of layers, N denotes the number of input sequences, and C denotes the number of chromosome classes.

The architecture and workflow of DNAMatch are illustrated in Fig. 1. The framework comprises two core modules: a pre-trained embedding module (Fig. 1B) and a chromosome identification module (Fig. 1C). In the embedding module, an input DNA fragment is first tokenized using Byte Pair Encoding (BPE). These tokens are then processed by the DNABERT-2 pre-trained model to extract high-dimensional feature representations. To obtain a compact, sequence-level representation, an average pooling operation is applied across the token-level embeddings, producing a global feature vector. This global representation is subsequently passed to the chromosome identification module, which uses a deep residual neural network (ResNet-based classifier) to predict the most likely chromosome of origin for the input sequence. Based on the classification result, Minimap2 is then employed to align the read against the predicted single-chromosome reference, achieving high-precision and resource-efficient alignment.
### A.	Pre-trained Embedding Module
Within the DNAMatch framework, the Pre-trained Embedding Module serves as the foundational component for feature engineering. Its principal objective is the transformation of raw DNA sequences from a symbolic format into high-dimensional numerical vectors. These vectors are designed to be rich representations that encapsulate the intrinsic genomic characteristics of the sequences. To achieve this, we leverage the expressive power of the pre-trained DNABERT-2 model.
### B.	Chromosome Identification Module
Following feature extraction, the chromosome recognition module classifies each DNA fragment pooled embedding to predict its chromosome of origin. This module is implemented as a deep residual network, which can be functionally divided into four stages: input preprocessing, deep feature learning, classification, and regularization.
